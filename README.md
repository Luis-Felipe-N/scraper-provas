# Scraper de Provas de Concursos

Scraper assÃ­ncrono para extraÃ§Ã£o de provas e gabaritos de concursos pÃºblicos do site PCI Concursos.

## ğŸ“‹ Funcionalidades

- **Scraping assÃ­ncrono** com `aiohttp` para alta performance
- **ExtraÃ§Ã£o de PDFs** de provas e gabaritos
- **Parsing de gabaritos** com mÃºltiplos formatos suportados
- **Retry automÃ¡tico** com backoff exponencial usando `tenacity`
- **Rate limiting** para evitar bloqueios
- **Suporte a mÃºltiplas bancas**: FGV, CEBRASPE, FCC, VUNESP, IBFC, CESGRANRIO, e mais

## ğŸš€ InstalaÃ§Ã£o

### PrÃ©-requisitos

- Python 3.11+
- pip ou poetry

### ConfiguraÃ§Ã£o

1. Clone o repositÃ³rio:

```bash
git clone https://github.com/Luis-Felipe-N/scraper-provas.git
cd scraper-provas
```

2. Crie e ative um ambiente virtual:

```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate  # Windows
```

3. Instale as dependÃªncias:

```bash
pip install -r requirements.txt
```

4. Configure as variÃ¡veis de ambiente (opcional):

```bash
cp .env.example .env
# Edite o arquivo .env conforme necessÃ¡rio
```

## âš™ï¸ ConfiguraÃ§Ã£o

O scraper pode ser configurado atravÃ©s de variÃ¡veis de ambiente ou arquivo `.env`:

| VariÃ¡vel | DescriÃ§Ã£o | PadrÃ£o |
|----------|-----------|--------|
| `SCRAPER_TIMEOUT` | Timeout das requisiÃ§Ãµes (segundos) | `10.0` |
| `SCRAPER_DELAY_BETWEEN_REQUESTS` | Delay entre requisiÃ§Ãµes (segundos) | `0.5` |
| `SCRAPER_MAX_CONCURRENT_REQUESTS` | MÃ¡ximo de requisiÃ§Ãµes simultÃ¢neas | `10` |
| `SCRAPER_RETRY_ATTEMPTS` | NÃºmero de tentativas em caso de erro | `3` |
| `SCRAPER_LOG_LEVEL` | NÃ­vel de log (DEBUG, INFO, WARNING, ERROR) | `INFO` |

## ğŸ“– Uso

### Uso BÃ¡sico

```python
import asyncio
import aiohttp
from src.scrapers import PciConcursosScraper
from src.services import PdfExtractor

async def main():
    scraper = PciConcursosScraper()
    extractor = PdfExtractor()
    
    base_url = "https://www.pciconcursos.com.br/provas/fgv"
    
    async with aiohttp.ClientSession(timeout=scraper.timeout) as session:
        async for exam in scraper.scrape_all(base_url):
            # Enriquecer com URLs de download
            await scraper.enrich_exam(session, exam)
            
            print(f"Prova: {exam.name}")
            print(f"Ano: {exam.year}")
            print(f"Ã“rgÃ£o: {exam.organization}")
            print(f"PDF da Prova: {exam.download.exam_url}")
            print(f"PDF do Gabarito: {exam.download.answer_key_url}")
            
            # Extrair gabarito
            if exam.download.answer_key_url:
                answer_keys = await extractor.extract_answer_keys_from_url(
                    session, exam.download.answer_key_url
                )
                
                for ak in answer_keys:
                    print(f"Gabarito: {ak.exam_name}")
                    print(f"Respostas: {ak.answers}")

asyncio.run(main())
```

### Executar o Script Principal

```bash
python scraper_main.py
```

### Extrair Texto de PDF

```python
from src.services import PdfExtractor

extractor = PdfExtractor()

# Extrair de arquivo local
with open("prova.pdf", "rb") as f:
    content = extractor.extract_text(f.read())
    print(content.text)
    print(f"PÃ¡ginas: {content.num_pages}")

# Extrair questÃµes
questions = extractor.extract_questions(content.text)
for q in questions:
    print(f"QuestÃ£o {q['number']}: {q['text'][:100]}...")
```

## ğŸ—ï¸ Estrutura do Projeto

```
scraper-provas/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py           # ConfiguraÃ§Ãµes e constantes
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ exam.py         # Modelos de dados (Exam, Download)
â”‚   â”œâ”€â”€ scrapers/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base.py         # Classe base abstrata
â”‚   â”‚   â””â”€â”€ pci_concursos.py # ImplementaÃ§Ã£o para PCI Concursos
â”‚   â””â”€â”€ services/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ pdf_extractor.py # ExtraÃ§Ã£o de PDFs
â”œâ”€â”€ scraper_main.py         # Script de exemplo
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â””â”€â”€ README.md
```

## ğŸ“¦ MÃ³dulos

### `src.scrapers`

- **`BaseScraper`**: Classe base abstrata com retry automÃ¡tico e rate limiting
- **`PciConcursosScraper`**: ImplementaÃ§Ã£o especÃ­fica para o site PCI Concursos

### `src.services`

- **`PdfExtractor`**: ExtraÃ§Ã£o de texto, imagens e gabaritos de PDFs
  - `extract_text()`: Extrai texto do PDF
  - `extract_questions()`: Identifica e extrai questÃµes
  - `extract_answer_keys()`: Extrai gabaritos com suporte a mÃºltiplos formatos

### `src.models`

- **`Exam`**: Representa uma prova de concurso
- **`Download`**: URLs de download da prova e gabarito

## ğŸ¯ Bancas Suportadas

| Banca | Status |
|-------|--------|
| FGV | âœ… Testado |
| CEBRASPE | âœ… Testado |
| FCC | âœ… Testado |
| VUNESP | âœ… Testado |
| IBFC | âœ… Testado |
| CESGRANRIO | âœ… Testado |
| CONSULPLAN | âœ… Testado |
| FUNDATEC | âœ… Testado |
| FUNDEP | âœ… Suportado |
| IDECAN | âœ… Suportado |
| AOCP | âœ… Suportado |
| QUADRIX | âœ… Suportado |

## ğŸ”§ Desenvolvimento

### Adicionar Nova Banca

1. Crie uma nova classe em `src/scrapers/`:

```python
from src.scrapers.base import BaseScraper

class NovaBancaScraper(BaseScraper):
    def parse_exam_list(self, html: str) -> list[Exam]:
        # Implementar parsing da lista de provas
        pass
    
    def parse_download_page(self, html: str) -> tuple[str | None, str | None]:
        # Implementar parsing da pÃ¡gina de download
        pass
    
    async def scrape_all(self, base_url: str) -> AsyncIterator[Exam]:
        # Implementar scraping completo
        pass
```

2. Registre no `__init__.py`

### Testes

```bash
# Executar testes
pytest

# Com cobertura
pytest --cov=src
```

## ğŸ“ LicenÃ§a

Este projeto Ã© apenas para fins educacionais. Respeite os termos de uso dos sites que vocÃª estÃ¡ acessando.

## ğŸ¤ ContribuiÃ§Ã£o

ContribuiÃ§Ãµes sÃ£o bem-vindas! Por favor, abra uma issue ou pull request.

## âš ï¸ Aviso Legal

Este scraper deve ser usado de forma responsÃ¡vel e Ã©tica. Certifique-se de:

- Respeitar os termos de uso do site
- NÃ£o sobrecarregar os servidores
- Usar delays apropriados entre requisiÃ§Ãµes
- NÃ£o redistribuir conteÃºdo protegido por direitos autorais
